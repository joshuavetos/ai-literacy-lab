# AI Literacy Lab

A modular curriculum for building epistemic rigor when working with language models. Each lesson pairs a concrete illusion with reproducible verification steps.

## Curriculum Overview
- [Module 01 — Introduction: The Fluency Illusion](01_Introduction/README.md)
- [Module 02 — Inside the Black Box: LLM Mechanics for Auditors](02_LLM_Mechanics_For_Auditors/README.md)
- [Module 03 — Prompt Engineering for Epistemic Rigor](03_Prompt_Engineering_For_Rigor/README.md)
- [Module 04 — Verification and Cross-Validation Methods](04_Verification_Methods/README.md)
- [Module 05 — Hallucination Detection and Correction](05_Hallucination_Detection/README.md)
- [Module 06 — Philosophical and Ethical Context](06_Philosophical_Context/README.md)
- [Module 07 — Case Studies: When Fluency Failed Publicly](07_Case_Studies/README.md)

## Community & Resources
- [Contributor Guide](08_Community/CONTRIBUTING.md)
- [Reading List](resources/reading_list.md)

## Sandbox Exercises
Hands-on verification scripts live under [`sandbox/`](sandbox). Start with `sandbox/01_fact_check_challenge/verify.py` to experience the fluency illusion firsthand.
