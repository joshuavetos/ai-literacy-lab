# Model Placement

Place a llama.cpp-compatible quantised model at `ggml-model-q4_0.bin` to enable live inference. The repository ships with a placeholder token file so the Docker image can still boot and replay the cached responses when no model is supplied.
